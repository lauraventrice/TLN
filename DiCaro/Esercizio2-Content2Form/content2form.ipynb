{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise “Content2form”\n",
    "\n",
    "Using the data on definitions (\"defs\" exercise), for each concept:\n",
    "- take the available definitions,\n",
    "- search WordNet for the correct synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully ✓\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.wsd import lesk\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully ✓\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read document definitions and create a data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs_path = f'../Esercizio1-DEFS/resource/definitions.csv'\n",
    "defs_path_json = f'../Esercizio1-DEFS/resource/definitions.json'\n",
    "slang_path = f'../Esercizio1-DEFS/resource/slang.txt'\n",
    "\n",
    "if not os.path.exists(defs_path_json):\n",
    "    with open(defs_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        definitions = []\n",
    "        for row in reader:\n",
    "            definitions.append(row)\n",
    "\n",
    "    with open(defs_path_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(definitions, f, indent=4)\n",
    "\n",
    "\n",
    "with open(defs_path_json, 'r', encoding='utf-8') as f: \n",
    "    definitions = json.load(f)\n",
    "\n",
    "\n",
    "with open(slang_path, 'r', encoding='utf-8') as f:\n",
    "    slang = f.read().splitlines()\n",
    "    slangs = []\n",
    "    for pair in slang: \n",
    "        list_pair = pair.split(\"=\")\n",
    "        slangs.append((list_pair[0].lower(), list_pair[1].lower()))\n",
    "\n",
    "def expand_slangs(tokens: list, slangs: list):\n",
    "    for i, token in enumerate(tokens):\n",
    "        for slang in slangs:\n",
    "            if token == slang[0]:\n",
    "                tokens[i] = slang[1]\n",
    "    return tokens\n",
    "\n",
    "def expand_abbr(tokens: list): \n",
    "    for i, token in enumerate(tokens): \n",
    "        if token == \"e.g.\" or token == \"eg\":\n",
    "            tokens[i] = \"for example\"\n",
    "        elif token == \"i.e.\" or token == \"ie\":\n",
    "            tokens[i] = \"that is\"\n",
    "        elif token == \"e.i.\" or token == \"ei\":\n",
    "            tokens[i] = \"for example that is\"\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  pre processing - stopwords removal, lemmatization, slang expansion, abbreviation expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokens_concepts = {}\n",
    "\n",
    "for concept in definitions:  \n",
    "    keys = list(concept.keys()) \n",
    "    keys.remove('Concept')\n",
    "    tokens = set()\n",
    "    for key in keys: # for all possible definition for the concept  \n",
    "        definition = concept[key].lower() \n",
    "        if definition != '':\n",
    "            def_tok = nltk.word_tokenize(definition)\n",
    "\n",
    "            def_tok = expand_slangs(def_tok, slangs) \n",
    "            def_tok = expand_abbr(def_tok) \n",
    "\n",
    "            def_tok = [token.lower() for token in def_tok if token not in stopwords and token.isalpha()]\n",
    "            lemmatizer = WordNetLemmatizer() \n",
    "            def_lem = [lemmatizer.lemmatize(token) for token in def_tok] \n",
    "            #tokens.update(def_lem) \n",
    "            concept[key] = def_lem  \n",
    "        else: \n",
    "            del concept[key] \n",
    "    \n",
    "    tokens_concepts[concept['Concept']] = list(tokens) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. filter definition that are too short -> create lexial material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept in definitions:\n",
    "    too_short = []\n",
    "    for key, value in concept.items():\n",
    "        if key != 'Concept':\n",
    "            if len(value) < 3:\n",
    "                too_short.append(key)\n",
    "    for key in too_short:\n",
    "        del concept[key]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find the right concept using wordnet and onomasiologic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(definitions: list, n_top: int) -> list: \n",
    "    concept_counter = Counter()\n",
    "    \n",
    "    for definition in definitions:\n",
    "        concept_counter.update(definition)\n",
    "    \n",
    "    most_frequent_words = [word for word, _ in concept_counter.most_common(n_top)]\n",
    "\n",
    "    print(\"top words:\", most_frequent_words, '\\n')\n",
    "\n",
    "    return list(set(most_frequent_words))\n",
    "\n",
    "def get_synset(word: str, definitions: list): \n",
    "    synset_counter = Counter()\n",
    "    for definition in definitions: \n",
    "        synset = lesk(definition, word, 'n')\n",
    "        if synset:\n",
    "            synset_counter.update([synset])\n",
    "\n",
    "    synset = None\n",
    "    if len(synset_counter) != 0:\n",
    "        synset = synset_counter.most_common(1)[0][0]\n",
    "\n",
    "    return synset\n",
    "\n",
    "def create_dict_word_dictionary(most_frequent_words: list, definitions: list):\n",
    "    dict_word_dictionary = {}\n",
    "    for word in most_frequent_words:\n",
    "        for definition in definitions: \n",
    "            if word in definition:\n",
    "                if word in dict_word_dictionary:\n",
    "                    dict_word_dictionary[word].append(definition)\n",
    "                else:\n",
    "                    dict_word_dictionary[word] = [definition]\n",
    "\n",
    "    return dict_word_dictionary\n",
    "\n",
    "def rank(top_words: list, important_words: list): \n",
    "    top_words_5 = top_words[:5]\n",
    "    top_words_10 = top_words[5:10]\n",
    "    top_words_end = top_words[10:]\n",
    "\n",
    "    rank = len(important_words)\n",
    "\n",
    "    count_5 = 0\n",
    "    count_10 = 0\n",
    "    count_end = 0\n",
    "\n",
    "    for word in important_words:\n",
    "        if word in top_words_5:\n",
    "            count_5 += 1\n",
    "        if word in top_words_10:\n",
    "            count_10 += 1\n",
    "        if word in top_words_end:\n",
    "            count_end += 1\n",
    "\n",
    "    return rank+(count_5)+(count_10*0.5)+(count_end*(-0.25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onomasiologic_search(concept: dict, n_top: int):\n",
    "    \n",
    "    results = []\n",
    "    del concept['Concept']\n",
    "    definitions = list(concept.values())\n",
    "\n",
    "    most_frequent_words = get_top_words(definitions, n_top)\n",
    "    dict_word_dictionary = create_dict_word_dictionary(most_frequent_words, definitions)\n",
    "\n",
    "    hypernyms = []\n",
    "    for word in most_frequent_words:\n",
    "        synset = get_synset(word, dict_word_dictionary[word])\n",
    "        if synset:\n",
    "            hypernyms.extend(synset.hypernyms())\n",
    "    \n",
    "    hypernyms = list(set(hypernyms))\n",
    "    res = []\n",
    "    for hyp in hypernyms:\n",
    "        hyp_def = hyp.definition() + \" \" + ', '.join(hyp.examples())\n",
    "        \n",
    "        match_words = []\n",
    "        for word in most_frequent_words:\n",
    "            if word in hyp_def:\n",
    "                match_words.append(word) \n",
    "        \n",
    "        res.append([hyp, match_words])\n",
    "\n",
    "     # sort the list using the number of important words found\n",
    "    sorted_res = sorted(res, key=lambda x: rank(most_frequent_words, x[1]), reverse=True)\n",
    "    for synset, match_words in sorted_res[:5]:\n",
    "        results.append((synset.name(), match_words))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "top words: ['feeling', 'human', 'feel', 'something', 'state', 'being', 'living', 'concept', 'certain', 'animal', 'sensation', 'mind', 'express', 'emotion', 'mental', 'range', 'situation', 'think', 'make', 'good', 'bad', 'arising', 'form', 'percieve', 'towards', 'others', 'sentiment', 'entity', 'throw', 'word'] \n",
      "\n",
      "[('feeling.n.01', ['feeling', 'emotion', 'state', 'feel']), ('body.n.01', ['animal', 'human', 'being']), ('emotional_state.n.01', ['emotion', 'state', 'good']), ('idea.n.01', ['good', 'think', 'mind']), ('living_thing.n.01', ['living', 'entity'])] \n",
      "\n",
      "\n",
      "\n",
      "Person\n",
      "top words: ['human', 'person', 'certain', 'ability', 'single', 'living', 'homo', 'sapiens', 'individual', 'answer', 'question', 'mean', 'may', 'say', 'generic', 'describe', 'precise', 'feature', 'belonging', 'group', 'society', 'mammal', 'descending', 'ape', 'entity', 'sentient', 'see', 'touch', 'member', 'specie'] \n",
      "\n",
      "[('causal_agent.n.01', ['entity']), ('organism.n.01', ['ability', 'living']), ('currency.n.01', ['ape']), ('person.n.01', ['human', 'person']), ('people.n.01', ['human', 'group'])] \n",
      "\n",
      "\n",
      "\n",
      "Revenge\n",
      "top words: ['someone', 'anger', 'feeling', 'action', 'reaction', 'act', 'something', 'emotion', 'person', 'hurting', 'done', 'bad', 'towards', 'wrong', 'negative', 'consequence', 'resulting', 'generally', 'arising', 'another', 'way', 'hurt', 'revenge', 'return', 'damaging', 'usually', 'wrongdoing', 'describes', 'classified', 'good'] \n",
      "\n",
      "[('return.n.10', ['return', 'action', 'act', 'good']), ('resistance.n.01', ['action', 'act', 'feeling', 'something']), ('injury.n.05', ['wrong', 'wrongdoing', 'another']), ('pain.n.02', ['feeling', 'emotion']), ('feeling.n.01', ['feeling', 'emotion'])] \n",
      "\n",
      "\n",
      "\n",
      "Brick\n",
      "top words: ['used', 'object', 'construction', 'material', 'build', 'building', 'made', 'clay', 'block', 'something', 'usually', 'brick', 'house', 'constructing', 'element', 'like', 'piece', 'shape', 'wall', 'aim', 'basic', 'parallelepiped', 'tool', 'resistnat', 'polygonal', 'different', 'size', 'red', 'generally', 'cunstruction'] \n",
      "\n",
      "[('building_material.n.01', ['material', 'build', 'constructing', 'used', 'building']), ('ceramic.n.01', ['material', 'made']), ('implement.n.01', ['tool', 'used', 'piece']), ('artifact.n.01', ['made', 'object']), ('filler.n.01', ['used'])] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for concept in definitions:\n",
    "    print(concept['Concept'])\n",
    "    result = onomasiologic_search(concept, 30)\n",
    "    print(result, '\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f6057f19fd601e680b310ee2ebe0fee3e78679207250b2f4d8f20eb0597a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
