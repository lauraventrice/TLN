{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \"Segmentation\"\n",
    "\n",
    "Implement a simple algorithm on text segmentation\n",
    "- use as a test an input of k paragraphs taken from different topics (e.g., Wikipedia pages)\n",
    "- is your system able to find the right \"cuts\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully ✓\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(\"Libraries imported successfully ✓\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load corpus - Paragraphs from different topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_corpus = f\"resource/corpus.txt\"\n",
    "\n",
    "pages = [\"New York City\", \"Machine Learning\", \"Vincent van Gogh\", \"Cubism\"]\n",
    "\n",
    "text = \"\"\n",
    "sentences_enumerate = []\n",
    "\n",
    "if not os.path.exists(file_corpus):\n",
    "    text_sentences = []\n",
    "    for page in pages: \n",
    "        text_topic_sentences = []\n",
    "        page_wiki = wiki_wiki.page(page)\n",
    "        print(\"Page - Title: %s\" % page_wiki.title)\n",
    "        text_topic = page_wiki.summary\n",
    "        sections = page_wiki.sections\n",
    "        for i in range(5): \n",
    "            text_topic += sections[i].text.replace(\"\\n\", \" \")\n",
    "        #print(\"Page - Summary: %s\" % summary)\n",
    "        if text == \"\":  \n",
    "            text = text_topic\n",
    "        else:\n",
    "            text = text + \"\\n\" + text_topic\n",
    "\n",
    "        text_topic_sentences = [sent.text.strip() for sent in nlp(text_topic).sents if sent.text.strip() != \"\"]\n",
    "\n",
    "        text_sentences.extend([' '.join(text_topic_sentences[i:i+4]) for i in range(0, len(text_topic_sentences), 4)])\n",
    "\n",
    "    sentences_enumerate = list(enumerate(text_sentences))\n",
    "\n",
    "    with open(file_corpus, \"w\", encoding='utf-8') as f:\n",
    "        for index, sentence in sentences_enumerate:\n",
    "            f.write(str(index) + \": \" + sentence + \"\\n\")\n",
    "\n",
    "else: \n",
    "    with open(file_corpus, \"r\", encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "        sentences_enumerate = []\n",
    "        for sentence in sentences: \n",
    "            index = sentence.split(\":\")[0]\n",
    "            sentence = sentence.split(\":\")[1]\n",
    "            sentences_enumerate.append((int(index), sentence))\n",
    "\n",
    "# cuts: [19, 35, 58]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sentences, pre processing and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sentences = {}\n",
    "words_text = set()\n",
    "for index, sentence in sentences_enumerate:\n",
    "    words_sentence = Counter()\n",
    "    words = nlp(sentence)\n",
    "    words_clean = [word.lemma_.lower() for word in words if word.is_alpha and word.text not in stopwords and word.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]]\n",
    "    \n",
    "    words_text.update(words_clean)\n",
    "    words_sentence.update(words_clean)\n",
    "\n",
    "    words_sentences[index] = words_sentence\n",
    "\n",
    "words_text = list(words_text) \n",
    "\n",
    "# complete encoding of the words in the sentences with 0s\n",
    "for sentence in words_sentences:    \n",
    "    words_count = list(words_sentences[sentence].keys())\n",
    "    for word in words_text: \n",
    "        if not word in words_count: \n",
    "            words_sentences[sentence][word] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Detection of words with distribution of frequency not uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(words_sentences)\n",
    "\n",
    "rows_to_drop = []\n",
    "for index, row in df.iterrows():\n",
    "    row_valued = row[row != 0]\n",
    "    sparsity = len(row_valued) / len(row)\n",
    "    if sparsity < 0.10 or sparsity > 0.90: # are not meaningful words (too used or too rare)\n",
    "        rows_to_drop.append(index)\n",
    "\n",
    "for row_to_drop in rows_to_drop: \n",
    "    df.drop(row_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  \\\n",
      "city        3   0   3   1   4   2   2   1   2   0   1   2   4   6   1   1   2   \n",
      "also        1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   \n",
      "center      1   2   0   0   0   0   1   0   1   0   0   0   0   0   0   1   1   \n",
      "large       1   1   1   1   1   0   0   2   1   1   0   0   0   1   0   1   0   \n",
      "world       2   2   2   2   0   2   3   1   4   0   0   0   0   0   1   1   0   \n",
      "make        0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "influence   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "become      0   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   0   \n",
      "sculpture   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "develop     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "century     0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   \n",
      "well        0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "use         0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "art         0   1   0   0   0   0   0   0   3   0   0   0   0   0   0   1   0   \n",
      "describe    0   1   0   0   0   0   1   0   0   0   0   0   0   0   1   0   0   \n",
      "learn       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "letter      0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "painting    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "many        0   0   1   0   0   1   1   0   0   0   0   0   0   1   0   1   0   \n",
      "know        0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   \n",
      "first       0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "learning    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "field       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "work        0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   \n",
      "movement    0   0   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   \n",
      "include     0   0   0   0   0   1   0   1   1   0   0   0   0   0   0   0   0   \n",
      "machine     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "early       0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "computer    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "time        0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   \n",
      "\n",
      "           17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  \\\n",
      "city        1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "also        1   0   0   0   1   0   0   1   0   0   0   0   0   0   0   0   0   \n",
      "center      1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "large       0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "world       0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "make        0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "influence   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   \n",
      "become      0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "sculpture   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "develop     0   0   1   0   0   0   1   1   0   0   0   0   0   0   0   0   0   \n",
      "century     1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "well        0   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "use         0   0   1   2   0   0   2   3   1   0   1   0   0   0   0   0   1   \n",
      "art         0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   \n",
      "describe    0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   \n",
      "learn       0   0   1   0   1   1   0   0   1   0   3   1   0   2   3   0   2   \n",
      "letter      0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   \n",
      "painting    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   \n",
      "many        0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "know        0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   \n",
      "first       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "learning    0   0   2   6   1   2   1   3   2   1   1   2   0   1   1   1   1   \n",
      "field       0   0   1   2   0   0   0   1   0   1   0   0   0   0   0   0   1   \n",
      "work        0   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "movement    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "include     1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "machine     0   0   2   4   1   1   2   2   2   1   2   1   0   0   1   0   3   \n",
      "early       0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   \n",
      "computer    0   0   1   1   0   3   2   2   1   0   1   1   0   0   0   0   0   \n",
      "time        0   0   0   0   0   0   0   1   0   0   0   0   0   3   1   0   0   \n",
      "\n",
      "           34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  \\\n",
      "city        0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "also        1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "center      0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "large       0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   \n",
      "world       0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   \n",
      "make        1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   \n",
      "influence   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   \n",
      "become      0   0   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   \n",
      "sculpture   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "develop     1   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "century     0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   \n",
      "well        0   0   0   0   0   0   0   1   0   1   0   0   0   0   1   1   0   \n",
      "use         2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "art         0   0   1   0   0   0   0   3   0   1   0   0   1   0   1   0   1   \n",
      "describe    0   0   0   0   0   0   0   0   1   0   0   1   0   1   0   0   0   \n",
      "learn       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "letter      0   0   0   1   0   0   0   3   2   2   2   3   1   0   0   1   0   \n",
      "painting    0   0   1   0   1   0   2   0   0   0   0   2   0   1   0   3   1   \n",
      "many        0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   \n",
      "know        0   0   0   0   0   0   0   1   0   0   0   0   2   0   0   1   0   \n",
      "first       1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   \n",
      "learning    4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "field       0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "work        0   0   1   3   0   0   1   0   0   0   0   1   0   3   2   1   1   \n",
      "movement    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "include     0   0   0   1   1   1   0   0   0   0   0   0   0   0   1   0   0   \n",
      "machine     4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "early       0   0   0   1   0   0   1   0   0   0   0   0   0   0   0   1   0   \n",
      "computer    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "time        0   0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "           51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  \\\n",
      "city        0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "also        0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   1   \n",
      "center      0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   \n",
      "large       1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "world       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   \n",
      "make        0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   \n",
      "influence   0   0   0   0   0   0   0   0   1   1   0   1   0   0   0   0   2   \n",
      "become      1   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   1   \n",
      "sculpture   0   0   0   0   0   0   0   1   0   1   0   1   1   1   1   0   1   \n",
      "develop     0   0   0   0   0   0   0   0   1   1   0   1   1   0   0   1   1   \n",
      "century     0   1   0   0   0   0   0   2   0   0   0   0   0   1   1   0   0   \n",
      "well        0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "use         0   0   0   0   1   0   0   1   0   0   0   1   0   0   0   3   0   \n",
      "art         1   1   0   1   0   0   0   3   1   0   2   0   0   0   1   0   0   \n",
      "describe    0   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   \n",
      "learn       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "letter      0   2   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "painting    0   0   1   0   0   1   0   1   1   1   0   2   1   0   1   0   1   \n",
      "many        0   0   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0   \n",
      "know        0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   \n",
      "first       0   1   0   0   0   0   0   0   0   0   1   0   0   1   0   0   0   \n",
      "learning    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "field       0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "work        2   0   0   0   0   1   0   0   2   0   0   1   0   1   0   0   0   \n",
      "movement    0   0   0   0   0   0   0   3   1   1   2   2   0   0   0   0   1   \n",
      "include     0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   \n",
      "machine     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "early       1   0   0   0   0   0   0   0   0   1   0   0   0   0   1   0   0   \n",
      "computer    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "time        0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0   \n",
      "\n",
      "           68  69  70  71  72  73  \n",
      "city        0   0   0   0   0   0  \n",
      "also        0   0   1   0   1   0  \n",
      "center      0   0   0   0   0   0  \n",
      "large       0   0   0   0   0   0  \n",
      "world       0   0   0   0   0   0  \n",
      "make        0   0   0   1   0   0  \n",
      "influence   0   1   2   1   1   0  \n",
      "become      0   0   0   0   0   0  \n",
      "sculpture   0   1   0   0   0   0  \n",
      "develop     0   0   0   0   0   0  \n",
      "century     0   0   0   0   0   0  \n",
      "well        1   0   1   0   1   0  \n",
      "use         0   0   0   0   0   0  \n",
      "art         0   0   0   0   0   1  \n",
      "describe    0   0   0   0   0   0  \n",
      "learn       0   0   0   0   0   0  \n",
      "letter      0   0   0   0   0   0  \n",
      "painting    2   1   0   0   0   0  \n",
      "many        0   1   0   0   0   0  \n",
      "know        1   0   0   0   0   0  \n",
      "first       0   0   1   0   0   0  \n",
      "learning    0   0   0   0   0   0  \n",
      "field       0   1   1   0   0   0  \n",
      "work        0   2   0   0   1   0  \n",
      "movement    0   0   0   1   0   0  \n",
      "include     0   1   0   0   0   0  \n",
      "machine     0   0   0   0   0   0  \n",
      "early       0   0   0   0   0   0  \n",
      "computer    0   0   0   0   0   0  \n",
      "time        0   0   0   0   0   0  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None,\n",
    "                   'display.max_columns', None,\n",
    "                   'display.precision', 3,\n",
    "                   ):\n",
    "    print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Text segmentation: \n",
    "\n",
    "- choose segments\n",
    "- measure of cohesion\n",
    "- change of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segments [23, 41, 59]\n"
     ]
    }
   ],
   "source": [
    "n_segments = 3\n",
    "segments = []\n",
    "width = len(df.columns) // (n_segments + 1)\n",
    "possible_cuts = list(df.columns[1:-2])\n",
    "\n",
    "for i in range(n_segments): \n",
    "    segment = (width * (i+1)) + 5\n",
    "    segments.append(segment)\n",
    "    possible_cuts.remove(segment)\n",
    "    if segment+1 in possible_cuts: \n",
    "        possible_cuts.remove(segment+1)\n",
    "    if segment-1 in possible_cuts:\n",
    "        possible_cuts.remove(segment-1)\n",
    "\n",
    "segments.sort()\n",
    "\n",
    "print(\"segments\", segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohesion_euclidean(centroid: np.ndarray, sentence: np.ndarray) -> float: \n",
    "    return np.linalg.norm(centroid - sentence)\n",
    "\n",
    "def cohesion_cosine(centroid: np.ndarray, sentence: np.ndarray) -> float:\n",
    "    return np.dot(centroid, sentence) / (norm(centroid) * norm(sentence))\n",
    "\n",
    "def intra_group_cohesion(df_segment: pd.DataFrame): \n",
    "    cohesion_value = 0\n",
    "    centroid_segment = df_segment.mean(axis=1).to_numpy()\n",
    "    for index in df_segment.columns: \n",
    "        sentence = df_segment[[index]].astype(float).to_numpy()\n",
    "        if np.count_nonzero(sentence) != 0:\n",
    "            cohesion_sentence = cohesion_cosine(centroid_segment, sentence)\n",
    "            cohesion_value += cohesion_sentence \n",
    "    return cohesion_value/len(df_segment.columns)\n",
    "\n",
    "def segmentation(df: pd.DataFrame, segments_df: list) -> list: \n",
    "    df_segmented = []\n",
    "    for segment_df in segments_df: # for each segment chosen\n",
    "        columns_to_drop = []\n",
    "        find = False\n",
    "        for col in df.columns:\n",
    "            if col == segment_df and not find:\n",
    "                find = True\n",
    "            if find:\n",
    "                columns_to_drop.append(col)\n",
    "        \n",
    "        df_new = df.drop(columns_to_drop, axis=1)\n",
    "        df = df.drop(df_new.columns, axis=1) \n",
    "        df_segmented.append(df_new)\n",
    "   \n",
    "    df_segmented.append(df)\n",
    "    return df_segmented\n",
    "\n",
    "def change_segmentation(segment1: pd.DataFrame, segment2: pd.DataFrame, reverse = False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    segment1_new = segment1.copy()\n",
    "    segment2_new = segment2.copy()\n",
    "    if len(segment1) > 1 and len(segment2) > 1: \n",
    "        if reverse:\n",
    "            first_column = segment2.columns[0]\n",
    "            segment1_new.insert(len(segment1.columns), first_column, segment2[first_column])\n",
    "            segment2_new.drop(first_column, axis=1, inplace=True)\n",
    "        else: \n",
    "            last_column = segment1.columns[-1]\n",
    "            segment1_new.drop(last_column, axis=1, inplace=True)\n",
    "            segment2_new.insert(0, last_column, segment1[[last_column]])\n",
    "    \n",
    "    return segment1_new, segment2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_segmentation(df: pd.DataFrame, segments: list): # main algorithm\n",
    "    to_continue = True\n",
    "    segments.sort()\n",
    "    while to_continue:    \n",
    "        new_segments = []\n",
    "        df_segmented = segmentation(df, segments)\n",
    "        #print(\"SEGMENTED:\", df_segmented)\n",
    "        pairs_segments = zip(df_segmented, df_segmented[1:], segments)\n",
    "        for segment1, segment2, value_segment in pairs_segments: # per tutte le porzioni di testo\n",
    "            # 6. intra-group cohesion in segments\n",
    "            cohesion1 = intra_group_cohesion(segment1)\n",
    "            cohesion2 = intra_group_cohesion(segment2)\n",
    "            print(\"COHESION:\", cohesion1, cohesion2, \"for:\", value_segment)\n",
    "\n",
    "            # 7. search of points with low cohesion -> break point\n",
    "            # 8. change of the segmentation \n",
    "            segment_new1, segment_new2 = change_segmentation(segment1, segment2)\n",
    "            cohesion1_new = intra_group_cohesion(segment_new1)\n",
    "            cohesion2_new = intra_group_cohesion(segment_new2)\n",
    "            print(\"COHESION NEW REVERSE FALSE: \", cohesion1_new, cohesion2_new)\n",
    "            if cohesion1_new > cohesion1 and cohesion2_new > cohesion2:\n",
    "                new_segments.append(value_segment+1)\n",
    "            else: \n",
    "                segment_new1, segment_new2 = change_segmentation(segment1, segment2, True)\n",
    "                cohesion1_new = intra_group_cohesion(segment_new1)\n",
    "                cohesion2_new = intra_group_cohesion(segment_new2)\n",
    "                print(\"COHESION NEW REVERSE TRUE: \", cohesion1_new, cohesion2_new)\n",
    "                if cohesion1_new > cohesion1 and cohesion2_new > cohesion2:\n",
    "                    new_segments.append(value_segment-1)\n",
    "                else: \n",
    "                    new_segments.append(value_segment)\n",
    "        \n",
    "        no_change = True\n",
    "        for value in segments: \n",
    "            no_change = no_change and value in new_segments\n",
    "\n",
    "        if no_change:\n",
    "            to_continue = False\n",
    "        else: \n",
    "            segments = new_segments\n",
    "            print(\"NEW SEGMENTS: \", segments)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHESION: [0.60970025] [0.51035119] for: 23\n",
      "COHESION NEW REVERSE FALSE:  [0.63129691] [0.51983006]\n",
      "COHESION: [0.51035119] [0.54218719] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.5290824] [0.53472281]\n",
      "COHESION NEW REVERSE TRUE:  [0.49216099] [0.52844334]\n",
      "COHESION: [0.54218719] [0.54914762] for: 59\n",
      "COHESION NEW REVERSE FALSE:  [0.5502074] [0.55317085]\n",
      "NEW SEGMENTS:  [24, 41, 60]\n",
      "COHESION: [0.59228514] [0.50233837] for: 24\n",
      "COHESION NEW REVERSE FALSE:  [0.60970025] [0.51035119]\n",
      "COHESION: [0.50233837] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.5207897] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.48423819] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "NEW SEGMENTS:  [25, 41, 60]\n",
      "COHESION: [0.57781874] [0.48856586] for: 25\n",
      "COHESION NEW REVERSE FALSE:  [0.59228514] [0.50233837]\n",
      "COHESION: [0.48856586] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.50634358] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.47091924] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "NEW SEGMENTS:  [26, 41, 60]\n",
      "COHESION: [0.56842541] [0.47097427] for: 26\n",
      "COHESION NEW REVERSE FALSE:  [0.57781874] [0.48856586]\n",
      "COHESION: [0.47097427] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.48700459] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.4529047] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "NEW SEGMENTS:  [27, 41, 60]\n",
      "COHESION: [0.56033983] [0.45963664] for: 27\n",
      "COHESION NEW REVERSE FALSE:  [0.56842541] [0.47097427]\n",
      "COHESION: [0.45963664] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.47419707] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.44253624] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "NEW SEGMENTS:  [28, 41, 60]\n",
      "COHESION: [0.55184177] [0.44437711] for: 28\n",
      "COHESION NEW REVERSE FALSE:  [0.56033983] [0.45963664]\n",
      "COHESION: [0.44437711] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.4560798] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.42854279] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "NEW SEGMENTS:  [29, 41, 60]\n",
      "COHESION: [0.54788398] [0.42909207] for: 29\n",
      "COHESION NEW REVERSE FALSE:  [0.55184177] [0.44437711]\n",
      "COHESION: [0.42909207] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.43693254] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.41416876] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "NEW SEGMENTS:  [30, 41, 60]\n",
      "COHESION: [0.52962118] [0.46810044] for: 30\n",
      "COHESION NEW REVERSE FALSE:  [0.54788398] [0.42909207]\n",
      "COHESION NEW REVERSE TRUE:  [0.51935736] [0.45411725]\n",
      "COHESION: [0.46810044] [0.54497064] for: 41\n",
      "COHESION NEW REVERSE FALSE:  [0.4806258] [0.53826456]\n",
      "COHESION NEW REVERSE TRUE:  [0.44868283] [0.53346874]\n",
      "COHESION: [0.54497064] [0.53483594] for: 60\n",
      "COHESION NEW REVERSE FALSE:  [0.54218719] [0.54914762]\n",
      "COHESION NEW REVERSE TRUE:  [0.53196256] [0.52137164]\n",
      "RISULTATI:  [30, 41, 60]\n"
     ]
    }
   ],
   "source": [
    "splits_def = text_segmentation(df, segments)\n",
    "print(\"RISULTATI: \", splits_def)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f6057f19fd601e680b310ee2ebe0fee3e78679207250b2f4d8f20eb0597a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
