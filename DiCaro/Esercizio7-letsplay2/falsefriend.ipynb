{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise : False Friends\n",
    "\n",
    "Text: Extraction of similar or identical words (as strings) but with different semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os.path\n",
    "from nltk.corpus import wordnet as wn\n",
    "from Levenshtein import ratio, hamming\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading of list of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus_ita = f'resource/ita/current_version_morph-it/morph-it_048_utf-8.txt'\n",
    "path_corpus_eng = f'resource/eng/BNC_lemmafile5.txt'\n",
    "\n",
    "path_words_ita = f'resource/ita/words_ita.txt'\n",
    "path_words_eng = f'resource/eng/words_eng.txt'\n",
    "\n",
    "\n",
    "pos_it = ['ADJ', 'ADV', 'ASP', 'AUX', 'CAU', 'MOD', 'NOUN', 'VER']\n",
    "words_ita = []\n",
    "pos_eng = ['j', 'v', 'n', 'r']\n",
    "words_eng = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian words loaded ✔ ( 16300 )\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Extraction of words from corpora ITA - Morph-It! (15k words)\n",
    "\n",
    "if not os.path.exists(path_words_ita) and os.path.exists(path_corpus_ita):\n",
    "    with open(path_corpus_ita, 'r', encoding='utf-8') as morph_it_file:\n",
    "        reader = morph_it_file.readlines()\n",
    "        for row in reader:\n",
    "            text, lemma, pos = re.split('\\t+', row)\n",
    "            if len(lemma) > 2 and lemma not in words_ita:\n",
    "                pos = pos.replace('\\n', '')\n",
    "                pos = pos.split(\":\")[0]\n",
    "                if pos in pos_it:\n",
    "                    words_ita.append(lemma)\n",
    "\n",
    "    with open(path_words_ita, 'w', encoding='utf-8') as words_ita_file:\n",
    "        for word in words_ita:\n",
    "            words_ita_file.write(word + '\\n')\n",
    "\n",
    "else: \n",
    "    with open(path_words_ita, 'r', encoding='utf-8') as words_ita_file:\n",
    "        reader = words_ita_file.readlines()\n",
    "        for row in reader:\n",
    "            words_ita.append(row.replace('\\n', ''))\n",
    "\n",
    "\n",
    "print(\"Italian words loaded ✔ (\", len(words_ita), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English words loaded ✔ ( 20280 )\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Extraction of words from corpora ENG - The British National Corpus (BNC) extracted from WordSmith Tools\n",
    "\n",
    "\n",
    "if not os.path.exists(path_words_eng) and os.path.exists(path_corpus_eng):\n",
    "    with open(path_corpus_eng, 'r', encoding='utf-8') as corpus_eng_file:\n",
    "        reader = corpus_eng_file.readlines()\n",
    "        for row in reader:\n",
    "            lemma = row.split(\" -> \")[0].lower()\n",
    "            if len(lemma) > 2 and lemma not in words_eng:\n",
    "                words_eng.append(lemma)\n",
    "\n",
    "    with open(path_words_eng, 'w', encoding='utf-8') as words_eng_file:\n",
    "        for word in words_eng:\n",
    "            words_eng_file.write(word + '\\n')\n",
    "\n",
    "else: \n",
    "    with open(path_words_eng, 'r', encoding='utf-8') as words_eng_file:\n",
    "        reader = words_eng_file.readlines()\n",
    "        for row in reader:\n",
    "            words_eng.append(row.replace('\\n', ''))\n",
    "\n",
    "print(\"English words loaded ✔ (\", len(words_eng), \")\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extraction of pairs of false-friends words from words_eng and words_ita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def semantic_similarity(word_eng: str, word_ita: str) -> float:\n",
    "    synsets_eng = wn.synsets(word_eng)\n",
    "    synsets_ita = wn.synsets(word_ita, lang='ita')\n",
    "    similarities = []\n",
    "    if len(synsets_eng) > 0 and len(synsets_ita) > 0:\n",
    "        for synset_eng in synsets_eng:\n",
    "            for synset_ita in synsets_ita:\n",
    "                similarities.append(wn.wup_similarity(synset_eng, synset_ita))\n",
    "    \n",
    "    return sum(similarities)/len(similarities) if len(similarities) > 0 else 1\n",
    "\n",
    "def char_similarity(word_eng: str, word_ita: str) -> float:\n",
    "    sim = 0\n",
    "    if len(word_eng) > 0 and len(word_ita) > 0:\n",
    "        if hamming(word_eng[:3], word_ita[:3]) <= 1: # check first 3 chars\n",
    "            sim = ratio(word_eng, word_ita)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of false-friends words extracted ✔ ( 20420 )\n"
     ]
    }
   ],
   "source": [
    "def extraction_pairs(words_eng: list, words_ita: list) -> list:\n",
    "    pairs = []\n",
    "    for word_eng in words_eng:\n",
    "        for word_ita in words_ita:\n",
    "            if char_similarity(word_eng, word_ita) > 0.7 and semantic_similarity(word_eng, word_ita) < 0.3: \n",
    "                pairs.append((word_eng, word_ita))\n",
    "    return pairs\n",
    "\n",
    "pairs = extraction_pairs(words_eng, words_ita)\n",
    "\n",
    "print(\"Pairs of false-friends words extracted ✔ (\", len(pairs), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of false-friends words saved ✔\n"
     ]
    }
   ],
   "source": [
    "# 3. Save pairs of false-friends words in a file\n",
    "path_false_friends = f'resource/result_false_friends.txt'\n",
    "with open(path_false_friends, 'w', encoding='utf-8') as false_friends_file:\n",
    "    for pair in pairs:\n",
    "        false_friends_file.write(pair[0] + '\\t' + pair[1] + '\\n')\n",
    "\n",
    "print(\"Pairs of false-friends words saved ✔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of false-friends words using EMBEDDINGS saved ✔\n"
     ]
    }
   ],
   "source": [
    "# BONUS: similarity between words using wordembeddings -> FastText\n",
    "\n",
    "fasttext.FastText.eprint = lambda x: None\n",
    "fasttext.util.download_model('it', if_exists='ignore')\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "model_eng = fasttext.load_model('cc.en.300.bin')\n",
    "model_ita = fasttext.load_model('cc.it.300.bin')\n",
    "\n",
    "def cosine_sim(u, v):\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        return np.dot(u, v) / (norm(u) * norm(v))\n",
    "\n",
    "def semantic_similarity_fasttext(word_eng: str, word_ita: str) -> float:\n",
    "    return cosine_sim(model_eng.get_word_vector(word_eng),(model_ita.get_word_vector(word_ita)))\n",
    "\n",
    "def extraction_pairs(words_eng: list, words_ita: list) -> list:\n",
    "    pairs = []\n",
    "    for word_eng in words_eng:\n",
    "        for word_ita in words_ita:\n",
    "            if char_similarity(word_eng, word_ita) > 0.7 and semantic_similarity_fasttext(word_eng, word_ita) < 0.3: \n",
    "                pairs.append((word_eng, word_ita))\n",
    "    return pairs\n",
    "\n",
    "pairs_emb = extraction_pairs(words_eng, words_ita)\n",
    "\n",
    "# 3. Save pairs of false-friends words in a file\n",
    "path_false_friends_emb = f'resource/result_false_friends_embeddings.txt'\n",
    "with open(path_false_friends_emb, 'w', encoding='utf-8') as false_friends_file:\n",
    "    for pair in pairs_emb:\n",
    "        false_friends_file.write(pair[0] + '\\t' + pair[1] + '\\n')\n",
    "\n",
    "print(\"Pairs of false-friends words using EMBEDDINGS saved ✔\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f6057f19fd601e680b310ee2ebe0fee3e78679207250b2f4d8f20eb0597a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
