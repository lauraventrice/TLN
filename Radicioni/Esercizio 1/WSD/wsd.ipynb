{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk \n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence).strip()\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords_file = open(f\"./resources/utils/stop_words__frakes_baeza-yates.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords_file.close()\n",
    "\n",
    "    stopwords_file = open(f\"./resources/utils/stop_words_1.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords_file.close()\n",
    "\n",
    "    stopwords_file = open(f\"./resources/utils/stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords_file.close()\n",
    "    \n",
    "    stopwords_list = list(set(stopwords_list))\n",
    "\n",
    "    return stopwords_list\n",
    "     \n",
    "def remove_stopwords(sentence, stopwords_list):\n",
    "    return [word for word in sentence.split() if word not in stopwords_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercitazione su Word Sense Disambiguation: \n",
    "\n",
    "#### INPUT : \n",
    "l’input per questa esercitazione è costituito da coppie di termini contenute nel file WordSim353 (disponibile nei\n",
    "formati .tsv e .csv)\n",
    "- Il file contiene 353 coppie di termini utilizzati come testset in varie competizioni internazionali\n",
    "- A ciascuna coppia è attribuito un valore numerico [0,10], che rappresenta la similarità fra gli elementi della coppia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353 correctly loaded!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "sim_file = f\"resources/WordSim353/WordSim353.csv\"\n",
    "\n",
    "pairs = []\n",
    "with open(sim_file, 'r', encoding = \"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()\n",
    "    for lines in reader:\n",
    "        pairs.append(lines)\n",
    "    \n",
    "print(\"WordSim353 correctly loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prima parte\n",
    "\n",
    "Implementare tre misure di similarità basate su WordNet. Per ciascuna delle misure di similarità, calcolare:\n",
    "- gli indici di correlazione di Spearman e\n",
    "- gli indici di correlazione di Pearson fra i risultati ottenuti e quelli ‘target’ presenti nel file annotato.\n",
    "\n",
    "Le misure da implementare sono le seguenti: \n",
    "- Wu & Palmer: \n",
    "$$cs(s_1, s_2) = \\frac{2 \\cdot depth(LCS)}{depth(s_1) + depth(s_2)}$$\n",
    "\n",
    "in cui $LCS$ rappresenta il primo antenato comune (Lowest Common Subsumer) e depth è la funzione che misura la distanza fra la radice di WordNet e il sysnet $x$. \n",
    "\n",
    "***L’obiettivo è implementare la misura di similarità di Wu & Palmer! Non dobbiamo fare ciò viene già fatto nell’implementazione di nltk o altre librerie!***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LCS(sysnet1, sysnet2):\n",
    "    hyperonyms1 = []\n",
    "    hyperonyms2 = []\n",
    "\n",
    "    hyperonyms1.append(sysnet1) # il primo elemento della lista è il synset da cui si parte, quindi la foglia\n",
    "    hyperonyms2.append(sysnet2) # il primo elemento della lista è il synset da cui si parte, quindi la foglia\n",
    "\n",
    "    # per ogni synset si prendono gli iperonimi e si aggiungono alla lista\n",
    "    for synset in hyperonyms1:\n",
    "        synset_hyp = synset.hypernyms()\n",
    "        #print(\"\\nPER \", str(synset), \" GLI IPERONIMI SONO: \", synset_hyp)\n",
    "\n",
    "        if synset_hyp:\n",
    "            hyperonyms1.extend(synset_hyp)\n",
    "\n",
    "    # per ogni synset si prendono gli iperonimi e si aggiungono alla lista\n",
    "    for synset in hyperonyms2:\n",
    "        synset_hyp = synset.hypernyms()\n",
    "        if synset_hyp:\n",
    "            hyperonyms2.extend(synset_hyp)\n",
    "\n",
    "    for syn1 in hyperonyms1:\n",
    "        for syn2 in hyperonyms2:\n",
    "            # il primo synset che è in comune è il più basso (LCS )\n",
    "            if syn1 == syn2:\n",
    "                return syn1 #LCS: Lowest Common Subsumer\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    oggetto\\n\\n    veicolo\\n\\n\\nbarca      automobile \\n\\n\\n            macchina\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def type_check(list_of_synset):\n",
    "    for syn in list_of_synset:\n",
    "        if not isinstance(syn, nltk.corpus.reader.wordnet.Synset):\n",
    "            raise TypeError(\"The input parameters must be Synset objects\")\n",
    "    return True\n",
    "\n",
    "def wu_palmer_similarity(syn1, syn2):\n",
    "    \"\"\"\n",
    "    Returns the Wu-Palmer similarity between synset.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(syn1, nltk.corpus.reader.wordnet.Synset) and isinstance(syn2, nltk.corpus.reader.wordnet.Synset):\n",
    "        print(\"\\nSYN1: \" + str(syn1))\n",
    "        print(\"SYN2: \" + str(syn2))\n",
    "        LCS = get_LCS(syn1, syn2)\n",
    "        print(\"LCS: \" + str(LCS))\n",
    "\n",
    "        lcs_depth = 0\n",
    "        \n",
    "        if LCS:\n",
    "            lcs_depth = LCS.max_depth()\n",
    "\n",
    "        # TODO: CAPIRE SE USARE MAX DEPTH O MIN DEPTH\n",
    "        cs = (2 * (lcs_depth)) / ((syn1.max_depth()) + (syn2.max_depth()))\n",
    "        \n",
    "        print(\"WE PALMER SIMILARITY MANUAL\")\n",
    "        print(cs)\n",
    "\n",
    "        print(\"WU PALMER SIMILARITY NLTK\")\n",
    "        print(syn1.wup_similarity(syn2))\n",
    "\n",
    "        return cs\n",
    "    else:\n",
    "        raise TypeError(\"The input parameters must be Synset objects\")\n",
    "    \n",
    "\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    oggetto\n",
    "\n",
    "    veicolo\n",
    "\n",
    "\n",
    "barca      automobile \n",
    "\n",
    "\n",
    "            macchina\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The input parameters must be list of Synset objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/DATI/GIT/TLN/Radicioni/Esercizio 1/WSD/wsd.ipynb Cella 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/DATI/GIT/TLN/Radicioni/Esercizio%201/WSD/wsd.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wu_palmer_similarity(wn\u001b[39m.\u001b[39;49msynsets(\u001b[39m\"\u001b[39;49m\u001b[39mboat\u001b[39;49m\u001b[39m\"\u001b[39;49m), wn\u001b[39m.\u001b[39;49msynsets(\u001b[39m\"\u001b[39;49m\u001b[39mcar\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32m/Volumes/DATI/GIT/TLN/Radicioni/Esercizio 1/WSD/wsd.ipynb Cella 8\u001b[0m in \u001b[0;36mwu_palmer_similarity\u001b[0;34m(syn1, syn2)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/DATI/GIT/TLN/Radicioni/Esercizio%201/WSD/wsd.ipynb#X10sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cs\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/DATI/GIT/TLN/Radicioni/Esercizio%201/WSD/wsd.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/DATI/GIT/TLN/Radicioni/Esercizio%201/WSD/wsd.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe input parameters must be list of Synset objects\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/DATI/GIT/TLN/Radicioni/Esercizio%201/WSD/wsd.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: The input parameters must be list of Synset objects"
     ]
    }
   ],
   "source": [
    "wu_palmer_similarity(wn.synsets(\"boat\"), wn.synsets(\"car\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shortest Path:\n",
    "$$sim_{path}(s_1, s_2) = 2 \\cdot depthMax - len(s_1, s_2)$$\n",
    "\n",
    "in cui $depthMax$ è un valore fissato per una specifica versione di WordNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCOLO REALE depthMax = max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())\n",
    "depth_max = 20\n",
    "\n",
    "# TODO: CONTROLLARE QUA -> C'E' QUALCOSA DA SISTEMARE\n",
    "def path_len_between_two_synset(synset1, synset2):\n",
    "    LCS = get_LCS(synset1, synset2)\n",
    "    print(\"LCS: \", LCS)\n",
    "    print(\"LCS LIBRARY: \", synset1.lowest_common_hypernyms(synset2))\n",
    "    if LCS:\n",
    "        print(\"synset1.max_depth(): \", synset1.max_depth())\n",
    "        print(\"synset2.max_depth(): \", synset2.max_depth())\n",
    "        print(\"LCS.max_depth(): \", LCS.max_depth())\n",
    "        \n",
    "        distance = (synset1.max_depth() - LCS.max_depth()) + (synset2.max_depth() - LCS.max_depth())\n",
    "        print(\"DISTANCE: \", distance)\n",
    "        print(\"DISTANCE LIBRARY: \", (synset1.shortest_path_distance(LCS) + synset2.shortest_path_distance(LCS)))\n",
    "\n",
    "        return distance\n",
    "\n",
    "def shortest_path(synset1, synset2):\n",
    "    return 2 * depth_max - path_len_between_two_synset(synset1, synset2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS:  Synset('whole.n.02')\n",
      "LCS LIBRARY:  [Synset('whole.n.02')]\n",
      "synset1.max_depth():  13\n",
      "synset2.max_depth():  8\n",
      "LCS.max_depth():  3\n",
      "DISTANCE:  15\n",
      "DISTANCE LIBRARY:  10\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_path_value = shortest_path(wn.synsets(\"dog\")[0], wn.synsets(\"computer\")[0])\n",
    "print(shortest_path_value)\n",
    "\n",
    "wn.synsets(\"dog\")[0].path_similarity(wn.synsets(\"cat\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Leakcock & Chodorow: \n",
    "$$sim_{LC}(s_1, s_2) = - log \\frac{len(s_1, s_2)}{2 \\cdot depthMax}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow(synset1, synset2):\n",
    "    fract = shortest_path(synset1, synset2) / (2 * depth_max)\n",
    "    return - math.log(fract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS:  Synset('whole.n.02')\n",
      "LCS LIBRARY:  [Synset('whole.n.02')]\n",
      "synset1.max_depth():  13\n",
      "synset2.max_depth():  8\n",
      "LCS.max_depth():  3\n",
      "DISTANCE:  15\n",
      "DISTANCE LIBRARY:  10\n",
      "MANUAL:  0.4700036292457356\n",
      "LIBRARY:  1.2396908869280152\n"
     ]
    }
   ],
   "source": [
    "print(\"MANUAL: \", leakcock_chodorow(wn.synsets(\"dog\")[0], wn.synsets(\"computer\")[0]))\n",
    "\n",
    "print(\"LIBRARY: \", wn.synsets(\"dog\")[0].lch_similarity(wn.synsets(\"computer\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcolo della similarità\n",
    "\n",
    "Le funzioni precedentemente implementate richiedono come input sensi e non i termini. Quindi per calcolare la similarità fra 2 termini è necessario prendere la massima similarità fra tutti i sensi del primo termine e tutti i sensi del secondo termine. \n",
    "\n",
    "L'ipotesi quindi è che i due termini funzionino come contesto di disambiguazione l'uno per l'altro. \n",
    "\n",
    "L'equazione che formalizza questa idea è la seguente: \n",
    "\n",
    "$$sim(w_1, w_2) = \\max_{c_1 \\in s(w_1), c_2 \\in s(w_2)} [sim(c_1, c_2)]$$\n",
    "\n",
    "***NB!*** Per calcolare gli indici di correlazione non si è interessati a entrare nel merito di come sono calcolati, possiamo prendere delle funzioni prefatte e usarle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seconda parte\n",
    "\n",
    "Implementare l’algoritmo di Lesk (NON (!=) usare implementazione esistente, e.g., in nltk…).\n",
    "1. Estrarre 50 frasi dal corpus SemCor (corpus annotato con i synset di\n",
    "WN) e disambiguare (almeno) un sostantivo per frase. \n",
    "Calcolare l’accuratezza del sistema implementato sulla base dei sensi annotati in\n",
    "SemCor.\n",
    "    - SemCor è disponibile all’URL http://web.eecs.umich.edu/~mihalcea/downloads.html\n",
    "2. Randomizzare la selezione delle 50 frasi e la selezione del termine da disambiguare, \n",
    "e restituire l’accuratezza media su (per esempio) 10 esecuzioni del programma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_of_words(phrase):\n",
    "    \"\"\"\n",
    "    Returns the set of words of a phrase.\n",
    "    \"\"\"\n",
    "    phrase = remove_punctuation(phrase)\n",
    "    set_of_words = remove_stopwords(phrase, stop_words)\n",
    "    \n",
    "    return set_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_of_phrase(sentence):\n",
    "    \"\"\"\n",
    "    Returns the context of a phrase.\n",
    "    \"\"\"\n",
    "    sentence = remove_punctuation(sentence)\n",
    "    set_of_words = remove_stopwords(sentence, stop_words)\n",
    "    return set_of_words\n",
    "\n",
    "def compute_overlap(signature, context):\n",
    "    \"\"\"\n",
    "    returns the number of words in common between signature and context\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_words_in_common = len(list(set(signature) & set(context)))\n",
    "\n",
    "    return number_of_words_in_common\n",
    "\n",
    "def get_signature(synset):\n",
    "    \"\"\"\n",
    "    returns the signature of synset\n",
    "    \"\"\"\n",
    "\n",
    "    gloss_of_synset = synset.definition() #gloss of synset\n",
    "    examples_of_synset = synset.examples() #examples of synset\n",
    "\n",
    "    # print(\"\\ngloss_of_synset\")\n",
    "    # print(gloss_of_synset)\n",
    "\n",
    "    # print(\"examples_of_synset\")\n",
    "    # print(examples_of_synset)\n",
    "\n",
    "    initial_signature = [gloss_of_synset] + examples_of_synset\n",
    "\n",
    "    signature = []\n",
    "    for phrase in initial_signature:\n",
    "        set_of_words = get_set_of_words(phrase)\n",
    "        signature.append(set_of_words)\n",
    "\n",
    "    result = sum(signature, []) # flatten list of lists\n",
    "    return list(set(result)) # remove duplicates\n",
    "\n",
    "def lesk(word, sentence):\n",
    "    \"\"\"\n",
    "    returns best sense of word\n",
    "    \"\"\"\n",
    "\n",
    "    best_sense = None #most frequent sense for word\n",
    "    max_overlap = 0\n",
    "    context = get_context_of_phrase(sentence) #set of words in sentence\n",
    "\n",
    "    print(\"CONTEXT:\")\n",
    "    print(context)\n",
    "\n",
    "    for synset in wn.synsets(word):\n",
    "        print(\"\\n\" + str(synset))\n",
    "        signature = get_signature(synset) #set of words in the gloss and examples of sense\n",
    "\n",
    "        print(\"SIGNATURE:\")\n",
    "        print(signature)\n",
    "\n",
    "        overlap = compute_overlap(signature, context)\n",
    "        print(\"OVERLAP: \" + str(overlap) + \"\\n\")\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = synset\n",
    "\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:\n",
      "['I', 'dog', 'park']\n",
      "\n",
      "Synset('dog.n.01')\n",
      "SIGNATURE:\n",
      "['domesticated', 'prehistoric', 'wolf', 'dog', 'member', 'man', 'times', 'barked', 'Canis', 'occurs', 'descended', 'genus', 'night', 'common', 'breeds']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('frump.n.01')\n",
      "SIGNATURE:\n",
      "['unattractive', 'woman', 'frump', 'dull', 'dog', 'reputation', 'unpleasant', 'real', 'girl']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('dog.n.03')\n",
      "SIGNATURE:\n",
      "['term', 'dog', 'informal', 'lucky', 'man']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('cad.n.01')\n",
      "SIGNATURE:\n",
      "['dirty', 'dog', 'reprehensible', 'morally']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('frank.n.02')\n",
      "SIGNATURE:\n",
      "['bread', 'beef', 'minced', 'pork', 'roll', 'smoked', 'smoothtextured', 'sausage', 'served']\n",
      "OVERLAP: 0\n",
      "\n",
      "\n",
      "Synset('pawl.n.01')\n",
      "SIGNATURE:\n",
      "['forward', 'moving', 'wheel', 'fits', 'prevent', 'backward', 'ratchet', 'catch', 'hinged', 'move', 'notch']\n",
      "OVERLAP: 0\n",
      "\n",
      "\n",
      "Synset('andiron.n.01')\n",
      "SIGNATURE:\n",
      "['hot', 'touch', 'supports', 'logs', 'andirons', 'metal', 'fireplace']\n",
      "OVERLAP: 0\n",
      "\n",
      "\n",
      "Synset('chase.v.01')\n",
      "SIGNATURE:\n",
      "['dog', 'chased', 'policeman', 'The', 'intent', 'alley', 'rabbit', 'mugger', 'catch']\n",
      "OVERLAP: 1\n",
      "\n",
      "best sense:\n",
      "Synset('dog.n.01')\n"
     ]
    }
   ],
   "source": [
    "best_sense = lesk(\"dog\", \"I saw a dog in the park\")\n",
    "\n",
    "print(\"best sense:\")\n",
    "print(best_sense) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1840fd1808eb491d43832dc3068188bb4de4861fc71046206c0ee2dff849d2e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
