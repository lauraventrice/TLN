{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "import os\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from py_babelnet.calls import BabelnetAPI\n",
    "\n",
    "KEY = '5ed5ed15-4bb7-4968-9d1d-eac164a4f95f'\n",
    "#KEY = 'b02f0cb7-4143-4e23-a581-6ff8b54bab88'\n",
    "babelnet = BabelnetAPI(KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consegna 1 - Annotazione di coppie di parole \n",
    "\n",
    "L'annotazione consiste in un punteggio di semantic similarity di 50 coppie di termini.\n",
    "\n",
    "Il criterio da utilizzare è presente al link https://tinyurl.com/y6f8h2kd. \n",
    "\n",
    "In particolare: \n",
    "- 4: **Molto simili** -- Le parole sono sinonimi (e.g., midday-noon). \n",
    "- 3: **Simili** -- Le parole condividono gran parte delle idee di significato ma includono dettagli profondamente differenti (e.g., lion-zebra). \n",
    "- 2: **Leggermente simili** -- Le parole non hanno significato molto simile ma condividono un argomento/dominio/funzione/idee/concetti che sono correlati (e.g., house-window). \n",
    "- 1: **Differenti** -- Le parole descrivono chiaramente concetti differenti, ma condividono qualche piccolo dettaglio come una lontana relazione o un dominio di utilizzo simile in un documento (e.g., software-keyboard).\n",
    "- 0: **Totalmente differenti e scorrelati** -- Le due parole non significano la stessa cosa e riguardano argomenti differenti (e.g., pencil-frog)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Estrazione di 50 coppie a partire dal cognome \"Scarpinati\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scarpinati     :\tcoppie nell'intervallo 251-300\n"
     ]
    }
   ],
   "source": [
    "def get_range(surname: str) -> int:\n",
    "    nof_elements = 500\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % 10)\n",
    "    idx_intervallo = base_idx * 50+1\n",
    "    return idx_intervallo\n",
    " \n",
    "\n",
    "input_name = \"Scarpinati\"\n",
    "\n",
    "values = []\n",
    "sx = get_range(input_name)\n",
    "values.append(sx)\n",
    "dx = sx+50-1\n",
    "intervallo = \"\" + str(sx) + \"-\" + str(dx)\n",
    "print('{:15}:\\tcoppie nell\\'intervallo {}'.format(input_name, intervallo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['crittogramma', 'simbolo'], ['campus', 'università'], ['libertà', 'libertà'], ['tifone', 'ciclone'], ['manuale', 'guida turistica'], ['Ebola', 'virus Ebola'], ['principessa', 'biscotto'], ['treno', 'tram'], ['ecosistema', 'economia'], ['regina regnante', 'regina degli scacchi'], ['oro', 'zinco'], ['cursore', 'patatine fritte'], ['Rio delle Amazzoni', 'foresta'], ['tennis', 'statistica'], ['Bolzano', 'teorema'], ['altitudine', 'conversione'], ['barca', 'albero'], ['trono', 'spada'], ['cotone', 'maglione'], ['ciliegia', 'fragola'], ['Islam', 'Corano'], ['Neanderthal', 'sport'], ['nepotismo', 're'], ['personaggio', 'persona'], ['cisterna', 'vagone'], ['Mercurio', 'Giove'], ['bronchite', 'acetaminofene'], ['sangue', 'corpo'], ['islamofobia', 'ISIS'], ['uncinetto', 'uniforme'], ['Hadoop', 'touchscreen'], ['combustibile fossile', 'fossile'], ['mutante', 'sociobiologia'], ['sorella', 'fratello'], ['matita', 'storia'], ['eczema', 'dermatite'], ['imperatore', 'governatore'], ['corona', 'chiave'], ['algebra', 'operazione'], ['pistone', 'motore'], ['borsa di studio', 'retta'], ['libro', 'manoscritto'], ['lantana', 'bocca di leone'], ['buco nero', 'vuoto'], ['Impero Persiano', 'Ciro'], ['editoriale', 'notizia'], ['decorazione', 'busta'], ['squalo balena', 'aragosta'], ['mais', 'granturco'], ['disturbo bipolare', 'problema']]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "path_corpus = f\"utils/it.test.data.txt\"\n",
    "\n",
    "with open(path_corpus, 'r', encoding='utf8') as file:\n",
    "    reader = file.readlines()[sx:dx+1]\n",
    "    pairs = [line.strip().split(\"\\t\") for line in reader]\n",
    "    print(pairs)\n",
    "    print(len(pairs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Memorizzazione delle coppie estratte\n",
    "\n",
    "**ATTENZIONE!** NON ESEGUIRE PIU' QUESTA CELLA, IL FILE E' GIA' STATO CREATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_extracted = f\"resources/it.test.data_extracted.tsv\"\n",
    "if not os.path.exists(path_extracted):\n",
    "    with open(path_extracted, 'w', newline='', encoding='utf8') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([\"Term1\", \"Term2\"])\n",
    "        tsv_writer.writerows(pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Estrazione dei valori di similarità inseriti per ogni coppia e calcolo della media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 3.0, 4.0, 4.0, 2.0, 4.0, 0.0, 3.0, 2.0, 2.0, 3.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 3.0, 3.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 3.0, 1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 2.0, 3.0, 0.0, 1.0, 1.0, 0.0, 3.0, 3.0, 0.0, 0.0, 1.0, 0.0, 3.0, 3.0, 2.0]\n",
      "[2.5, 3.2, 4.0, 4.0, 3.0, 3.5, 0.2, 3.5, 2.3, 0.0, 3.2, 0.0, 2.5, 0.5, 0.0, 0.0, 1.2, 2.5, 3.2, 3.3, 3.0, 0.2, 2.2, 3.2, 1.2, 3.2, 1.5, 3.3, 1.2, 1.3, 0.1, 2.2, 0.1, 3.6, 0.3, 2.7, 3.3, 0.2, 3.0, 3.0, 1.2, 3.7, 3.4, 3.0, 1.3, 1.2, 1.0, 3.2, 3.7, 1.5]\n",
      "[2.75, 3.1, 4.0, 4.0, 2.5, 3.75, 0.1, 3.25, 2.15, 1.0, 3.1, 0.0, 2.75, 0.25, 0.0, 0.0, 1.1, 2.75, 3.1, 3.15, 2.5, 0.1, 2.1, 2.6, 0.6, 3.1, 1.25, 2.65, 1.1, 1.15, 0.05, 1.1, 0.05, 3.3, 0.15, 2.35, 3.15, 0.1, 2.0, 2.0, 0.6, 3.35, 3.2, 1.5, 0.65, 1.1, 0.5, 3.1, 3.35, 1.75]\n",
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "similarity_livio = []\n",
    "similarity_laura = []\n",
    "mean_similarity = []\n",
    "\n",
    "path_annotated = f\"resources/part1/it.test.data_annotated.tsv\"\n",
    "\n",
    "if os.path.exists(path_annotated):\n",
    "    with open(path_annotated, 'r', encoding='utf8') as file:\n",
    "        tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "        header = next(tsv_file)\n",
    "        for line in tsv_file:\n",
    "            similarity_livio.append(float(line[2]))\n",
    "            similarity_laura.append(float(line[3]))\n",
    "\n",
    "mean_similarity = [(similarity_livio[i] + similarity_laura[i])/2 for i in range(50)]\n",
    "\n",
    "print(similarity_livio)\n",
    "print(similarity_laura)\n",
    "print(mean_similarity)\n",
    "\n",
    "print(len(similarity_laura))\n",
    "print(len(similarity_livio))\n",
    "print(len(mean_similarity))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Memorizzazione del documento con il valore medio di valutazione per ogni coppia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(path_annotated)\n",
    "\n",
    "with open(path_annotated, 'w', newline='', encoding='utf8') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"Sim_Livio\", \"Sim_Laura\", \"Mean\"])\n",
    "    for pair, sim_livio, sim_laura, mean in zip(pairs, similarity_livio, similarity_laura, mean_similarity):\n",
    "        tsv_writer.writerow([pair[0], pair[1], sim_livio, sim_laura, mean])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Calcolare agreement fra gli annotatori (inter-rater agreement)\n",
    "\n",
    "Vengono utilizzati gli indici di correlazione di **Pearson** e **Spearman**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Inter Agreement with Pearson & Spearman:\n",
      "Pearson \n",
      "Coefficiente di correlazione: 0.824495945350208\n",
      "p-value:  1.8586060579974576e-13\n",
      "\n",
      "\n",
      "Spearman \n",
      "Coefficiente di correlazione: 0.8401818319529221\n",
      "p-value:  2.3703924660488e-14\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Inter Agreement with Pearson & Spearman:\")\n",
    "pearson = pearsonr(similarity_livio, similarity_laura)\n",
    "spearman = spearmanr(similarity_livio, similarity_laura)\n",
    "print(\"Pearson \")\n",
    "print(\"Coefficiente di correlazione:\", pearson[0])\n",
    "print(\"p-value: \", pearson[1])\n",
    "print(\"\\n\\nSpearman \")\n",
    "print(\"Coefficiente di correlazione:\", spearman.correlation)\n",
    "print(\"p-value: \", spearman.pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 Valutazione\n",
    "\n",
    "La valutazione dei punteggi annotati è in rapporto alla similarità ottenuta utilizzando i vettori NASARI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nasari_vectors() -> dict: \n",
    "    path_nasari = f\"utils/mini_NASARI.tsv\"\n",
    "    nasari_vectors = {}\n",
    "    with open(path_nasari, 'r', encoding='utf8') as file:\n",
    "        tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "        for line in tsv_file:\n",
    "            synset_id, synset_name = line[0].split(\"__\")\n",
    "            vector = [float(value) for value in line[1:]]\n",
    "            nasari_vectors[synset_id] = vector\n",
    "    return nasari_vectors\n",
    "\n",
    "def get_senses2synsets() -> dict: \n",
    "    path_senses2synsets = f\"utils/SemEval17_IT_senses2synsets.txt\"\n",
    "    senses2synsets = {}\n",
    "    with open(path_senses2synsets, 'r', encoding='utf8') as file:\n",
    "        reader = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(reader):\n",
    "            if reader[i].startswith(\"#\"):\n",
    "                synset_name = reader[i][1:].strip()\n",
    "                synset_ids = []\n",
    "                i += 1\n",
    "                while i < len(reader) and not reader[i].startswith(\"#\"):\n",
    "                    synset_ids.append(reader[i].strip())\n",
    "                    i += 1\n",
    "                senses2synsets[synset_name] = synset_ids\n",
    "    return senses2synsets\n",
    "\n",
    "def get_synsets(word: str, nasari_vectors: dict, senses2synsets: dict) -> list[tuple[str, list[float]]]:\n",
    "    synsets = []\n",
    "    if word in senses2synsets:\n",
    "        for synset_id in senses2synsets[word]:\n",
    "            if synset_id in nasari_vectors:\n",
    "                synsets.append((synset_id, nasari_vectors[synset_id]))\n",
    "    return synsets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il calcolo della similarità è stata utilizzata la *cosine similarity*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(v1: list, v2: list) -> float:\n",
    "    return 1 - cosine(v1, v2)\n",
    "\n",
    "\n",
    "def get_similarity(word1: str, word2: str, nasari_vectors: dict, senses2synsets: dict) -> tuple[float, tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Restituisce la similarità massima tra i vettori NASARI dei due termini e vettori nasari che la massimizzano, se presenti.\n",
    "    Altrimenti restituisce 0.\n",
    "    \"\"\"\n",
    "    synsets1 = get_synsets(word1, nasari_vectors, senses2synsets)\n",
    "    synsets2 = get_synsets(word2, nasari_vectors, senses2synsets)\n",
    "    max_similarity = 0\n",
    "    max_similarity_ids = (\"\", \"\")\n",
    "    if len(synsets1) > 0 or len(synsets2) > 0:\n",
    "        for id1, v1 in synsets1:\n",
    "            for id2, v2 in synsets2:\n",
    "                similarity = get_cosine_similarity(v1, v2)\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    max_similarity_ids = (id1, id2)\n",
    "                    \n",
    "    return max_similarity, max_similarity_ids\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crittogramma simbolo\n",
      "Similarity: 0.624\n",
      "campus università\n",
      "Similarity: 0.9408\n",
      "libertà libertà\n",
      "Similarity: 1\n",
      "tifone ciclone\n",
      "Similarity: 1\n",
      "manuale guida turistica\n",
      "Similarity: 0.8345\n",
      "Ebola virus Ebola\n",
      "Similarity: 0.993\n",
      "principessa biscotto\n",
      "Similarity: 0.3956\n",
      "treno tram\n",
      "Similarity: 0.8679\n",
      "ecosistema economia\n",
      "Similarity: 0.4969\n",
      "regina regnante regina degli scacchi\n",
      "Similarity: 0\n",
      "oro zinco\n",
      "Similarity: 0.8507\n",
      "cursore patatine fritte\n",
      "Similarity: 0.3544\n",
      "Rio delle Amazzoni foresta\n",
      "Similarity: 0.6174\n",
      "tennis statistica\n",
      "Similarity: 0.6604\n",
      "Bolzano teorema\n",
      "Similarity: 0.5703\n",
      "altitudine conversione\n",
      "Similarity: 0.7004\n",
      "barca albero\n",
      "Similarity: 0.9279\n",
      "trono spada\n",
      "Similarity: 0.5891\n",
      "cotone maglione\n",
      "Similarity: 0.6254\n",
      "ciliegia fragola\n",
      "Similarity: 0.9073\n",
      "Islam Corano\n",
      "Similarity: 0.8843\n",
      "Neanderthal sport\n",
      "Similarity: 0.5034\n",
      "nepotismo re\n",
      "Similarity: 0.654\n",
      "personaggio persona\n",
      "Similarity: 1\n",
      "cisterna vagone\n",
      "Similarity: 0.5377\n",
      "Mercurio Giove\n",
      "Similarity: 1\n",
      "bronchite acetaminofene\n",
      "Similarity: 0.6769\n",
      "sangue corpo\n",
      "Similarity: 0.7127\n",
      "islamofobia ISIS\n",
      "Similarity: 0.7206\n",
      "uncinetto uniforme\n",
      "Similarity: 0.6059\n",
      "Hadoop touchscreen\n",
      "Similarity: 0.4627\n",
      "combustibile fossile fossile\n",
      "Similarity: 0.4849\n",
      "mutante sociobiologia\n",
      "Similarity: 0.695\n",
      "sorella fratello\n",
      "Similarity: 1\n",
      "matita storia\n",
      "Similarity: 0.6971\n",
      "eczema dermatite\n",
      "Similarity: 0.9712\n",
      "imperatore governatore\n",
      "Similarity: 0.9\n",
      "corona chiave\n",
      "Similarity: 0.8612\n",
      "algebra operazione\n",
      "Similarity: 0.8363\n",
      "pistone motore\n",
      "Similarity: 0.9808\n",
      "borsa di studio retta\n",
      "Similarity: 0.8931\n",
      "libro manoscritto\n",
      "Similarity: 0.829\n",
      "lantana bocca di leone\n",
      "Similarity: 0.8628\n",
      "buco nero vuoto\n",
      "Similarity: 0.9068\n",
      "Impero Persiano Ciro\n",
      "Similarity: 0.7807\n",
      "editoriale notizia\n",
      "Similarity: 0.9319\n",
      "decorazione busta\n",
      "Similarity: 0.5828\n",
      "squalo balena aragosta\n",
      "Similarity: 0.8194\n",
      "mais granturco\n",
      "Similarity: 1\n",
      "disturbo bipolare problema\n",
      "Similarity: 0.4845\n"
     ]
    }
   ],
   "source": [
    "nasari_vectors = get_nasari_vectors()\n",
    "senses2synsets = get_senses2synsets()\n",
    "\n",
    "similarities_nasari = []\n",
    "for word1, word2 in pairs: \n",
    "    print(word1, word2)\n",
    "    similarity, _ = get_similarity(word1, word2, nasari_vectors, senses2synsets)\n",
    "    similarities_nasari.append(round(similarity, 4))\n",
    "    print(\"Similarity:\", round(similarity, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confronto risultati medi dell'annotazione con i risultati ottenuti utilizzando la risorsa NASARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### word1  word2   norm mean similarity    nasari similarity:\n",
      "\n",
      "\n",
      "['crittogramma', 'simbolo']\t0.6875\t\t0.624\n",
      "['campus', 'università']\t0.775\t\t0.9408\n",
      "['libertà', 'libertà']\t1.0\t\t1\n",
      "['tifone', 'ciclone']\t1.0\t\t1\n",
      "['manuale', 'guida turistica']\t0.625\t\t0.8345\n",
      "['Ebola', 'virus Ebola']\t0.9375\t\t0.993\n",
      "['principessa', 'biscotto']\t0.025\t\t0.3956\n",
      "['treno', 'tram']\t0.8125\t\t0.8679\n",
      "['ecosistema', 'economia']\t0.5375\t\t0.4969\n",
      "['regina regnante', 'regina degli scacchi']\t0.25\t\t0\n",
      "['oro', 'zinco']\t0.775\t\t0.8507\n",
      "['cursore', 'patatine fritte']\t0.0\t\t0.3544\n",
      "['Rio delle Amazzoni', 'foresta']\t0.6875\t\t0.6174\n",
      "['tennis', 'statistica']\t0.0625\t\t0.6604\n",
      "['Bolzano', 'teorema']\t0.0\t\t0.5703\n",
      "['altitudine', 'conversione']\t0.0\t\t0.7004\n",
      "['barca', 'albero']\t0.275\t\t0.9279\n",
      "['trono', 'spada']\t0.6875\t\t0.5891\n",
      "['cotone', 'maglione']\t0.775\t\t0.6254\n",
      "['ciliegia', 'fragola']\t0.7875\t\t0.9073\n",
      "['Islam', 'Corano']\t0.625\t\t0.8843\n",
      "['Neanderthal', 'sport']\t0.025\t\t0.5034\n",
      "['nepotismo', 're']\t0.525\t\t0.654\n",
      "['personaggio', 'persona']\t0.65\t\t1\n",
      "['cisterna', 'vagone']\t0.15\t\t0.5377\n",
      "['Mercurio', 'Giove']\t0.775\t\t1\n",
      "['bronchite', 'acetaminofene']\t0.3125\t\t0.6769\n",
      "['sangue', 'corpo']\t0.6625\t\t0.7127\n",
      "['islamofobia', 'ISIS']\t0.275\t\t0.7206\n",
      "['uncinetto', 'uniforme']\t0.2875\t\t0.6059\n",
      "['Hadoop', 'touchscreen']\t0.0125\t\t0.4627\n",
      "['combustibile fossile', 'fossile']\t0.275\t\t0.4849\n",
      "['mutante', 'sociobiologia']\t0.0125\t\t0.695\n",
      "['sorella', 'fratello']\t0.825\t\t1\n",
      "['matita', 'storia']\t0.0375\t\t0.6971\n",
      "['eczema', 'dermatite']\t0.5875\t\t0.9712\n",
      "['imperatore', 'governatore']\t0.7875\t\t0.9\n",
      "['corona', 'chiave']\t0.025\t\t0.8612\n",
      "['algebra', 'operazione']\t0.5\t\t0.8363\n",
      "['pistone', 'motore']\t0.5\t\t0.9808\n",
      "['borsa di studio', 'retta']\t0.15\t\t0.8931\n",
      "['libro', 'manoscritto']\t0.8375\t\t0.829\n",
      "['lantana', 'bocca di leone']\t0.8\t\t0.8628\n",
      "['buco nero', 'vuoto']\t0.375\t\t0.9068\n",
      "['Impero Persiano', 'Ciro']\t0.1625\t\t0.7807\n",
      "['editoriale', 'notizia']\t0.275\t\t0.9319\n",
      "['decorazione', 'busta']\t0.125\t\t0.5828\n",
      "['squalo balena', 'aragosta']\t0.775\t\t0.8194\n",
      "['mais', 'granturco']\t0.8375\t\t1\n",
      "['disturbo bipolare', 'problema']\t0.4375\t\t0.4845\n"
     ]
    }
   ],
   "source": [
    "def normalize_range(values: list, min: int, max: int) -> list:\n",
    "    \"\"\"\n",
    "    Normalizza i valori in [0, 1] a partire dal range di partenza [min, max]\n",
    "    \"\"\"\n",
    "    normalized_values = [(val-min)/(max - min) for val in values]    \n",
    "    return normalized_values\n",
    "\n",
    "mean_similarity = normalize_range(mean_similarity, 0, 4)\n",
    "\n",
    "print(\"##### word1  word2   norm mean similarity    nasari similarity:\\n\\n\")\n",
    "\n",
    "for i in range(len(pairs)): \n",
    "    print(str(pairs[i]) + \"\\t\" + str(mean_similarity[i]) + \"\\t\\t\" + str(similarities_nasari[i]))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7 Coefficienti di correlazione tra la media degli annotatori e i risultati di NASARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Intra Agreement with Pearson & Spearman:\n",
      "Pearson \n",
      "Coefficiente di correlazione: 0.5587957118302855\n",
      "p-value:  2.469792546476724e-05\n",
      "\n",
      "\n",
      "Spearman \n",
      "Coefficiente di correlazione: 0.6038348992844942\n",
      "p-value:  3.4431691409166004e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Intra Agreement with Pearson & Spearman:\")\n",
    "\n",
    "pearson = pearsonr(mean_similarity, similarities_nasari)\n",
    "spearman = spearmanr(mean_similarity, similarities_nasari)\n",
    "\n",
    "print(\"Pearson \")\n",
    "print(\"Coefficiente di correlazione:\", pearson[0])\n",
    "print(\"p-value: \", pearson[1])\n",
    "print(\"\\n\\nSpearman \")\n",
    "print(\"Coefficiente di correlazione:\", spearman.correlation)\n",
    "print(\"p-value: \", spearman.pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consegna 2 - Sense Identification\n",
    "\n",
    "L'obiettivo è di individuare i sensi selezionati utilizzati per generare il giudizio di similarità nella consegna precedente.\n",
    "\n",
    "La domanda che ci poniamo è la seguente: *quali sensi abbiamo effettivamente utilizzato quando abbiamo assegnato un valore di similarità a una coppia di termini (per esempio, società e cultura)*?\n",
    "\n",
    "Per risolvere questo compito partiamo dall’assunzione che i due termini funzionino come contesto di disambiguazione l’uno per l’altro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Caricamento del file annotato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ids_laura = f\"resources/part2/it.test.data_ids_laura.tsv\"\n",
    "path_ids_livio = f\"resources/part2/it.test.data_ids_livio.tsv\"\n",
    "path_ids_NASARI = f\"resources/part2/it.test.data_ids_NASARI.tsv\"\n",
    "\n",
    "ids_laura : list[tuple[str, str]] = []\n",
    "ids_livio : list[tuple[str, str]] = []\n",
    "\n",
    "with open(path_ids_laura, 'r', encoding='utf8') as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    next(tsv_file)\n",
    "    for line in tsv_file:\n",
    "        ids_laura.append((line[2], line[3]))\n",
    "\n",
    "with open(path_ids_livio, 'r', encoding='utf8') as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    next(tsv_file)\n",
    "    for line in tsv_file:\n",
    "        ids_livio.append((line[2], line[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms_synsets(pairs_ids: list[tuple[str, str]]) -> list[tuple[list, list]]:\n",
    "    \"\"\"\n",
    "    Restituisce i termini associati ai synset di ogni coppia di termini a partire dai loro id.\n",
    "    \"\"\"\n",
    "    terms_synsets = []\n",
    "    for id1, id2 in pairs_ids:\n",
    "        terms_word1 = set()\n",
    "        terms_word2 = set()\n",
    "        \n",
    "        if id1 != \"\":\n",
    "            synset_word1 = babelnet.get_synset(id=id1, searchLang=\"IT\", targetLang=\"IT\")\n",
    "    \n",
    "            if \"senses\" in synset_word1:\n",
    "                for sense in synset_word1['senses']: \n",
    "                    terms_word1.add(sense[\"properties\"][\"fullLemma\"])\n",
    "        \n",
    "        if id2 != \"\":\n",
    "            synset_word2 = babelnet.get_synset(id=id2, searchLang=\"IT\", targetLang=\"IT\")\n",
    "            if \"senses\" in synset_word2:\n",
    "                for sense in synset_word2['senses']: \n",
    "                    terms_word2.add(sense[\"properties\"][\"fullLemma\"])\n",
    "        \n",
    "        terms_synsets.append((list(terms_word1), list(terms_word2)))\n",
    "\n",
    "    return terms_synsets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 Memorizzazione dei termini per ogni synset inserito nell'annotazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['criptogramma', 'crittogramma', 'crittogrammi'], ['simbolo', 'segno', 'simbologia', 'simboli', 'simbolico']), (['cittadella_universitaria', 'Campus_universitario', 'città_universitaria', 'Cittadella_universitaria', 'campus_universitario', 'campus'], ['ateneo', 'università']), (['autodeterminazione', 'libertà_personale', 'sovranità', 'Frēodōm', 'indipendenza', 'libertà_individuale', 'libertà'], ['autodeterminazione', 'libertà_personale', 'sovranità', 'Frēodōm', 'indipendenza', 'libertà_individuale', 'libertà'])]\n"
     ]
    }
   ],
   "source": [
    "terms_ids_laura = get_terms_synsets(ids_laura)\n",
    "\n",
    "print(terms_ids_laura[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['criptogramma', 'crittogramma', 'crittogrammi'], ['simbolo', 'segno', 'simbologia', 'simboli', 'simbolico']), (['cittadella_universitaria', 'Campus_universitario', 'città_universitaria', 'Cittadella_universitaria', 'campus_universitario', 'campus'], ['ateneo', 'università']), (['autodeterminazione', 'libertà_personale', 'sovranità', 'Frēodōm', 'indipendenza', 'libertà_individuale', 'libertà'], ['autodeterminazione', 'libertà_personale', 'sovranità', 'Frēodōm', 'indipendenza', 'libertà_individuale', 'libertà'])]\n"
     ]
    }
   ],
   "source": [
    "terms_ids_livio = get_terms_synsets(ids_livio)\n",
    "\n",
    "print(terms_ids_livio[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiornamento file annotati con i termini associati ai synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(path_ids_laura)\n",
    "\n",
    "with open(path_ids_laura, 'w', newline='', encoding='utf8') as file:\n",
    "    tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"BS1\", \"BS2\", \"Terms_in_BS1\", \"Terms_in_BS2\"])\n",
    "    for pair, id_laura, terms_id_laura in zip(pairs, ids_laura, terms_ids_laura):\n",
    "        terms_id0 = ','.join(terms_id_laura[0][:3])\n",
    "        terms_id1 = ','.join(terms_id_laura[1][:3])\n",
    "        tsv_writer.writerow([pair[0], pair[1], id_laura[0], id_laura[1], terms_id0, terms_id1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(path_ids_livio)\n",
    "\n",
    "with open(path_ids_livio, 'w', newline='', encoding='utf8') as file:\n",
    "    tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"BS1\", \"BS2\", \"Terms_in_BS1\", \"Terms_in_BS2\"])\n",
    "    for pair, id_livio, terms_id_livio in zip(pairs, ids_livio, terms_ids_livio):\n",
    "        terms_id0 = ','.join(terms_id_livio[0][:3])\n",
    "        terms_id1 = ','.join(terms_id_livio[1][:3])\n",
    "        tsv_writer.writerow([pair[0], pair[1], id_livio[0], id_livio[1], terms_id0, terms_id1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Coefficiente di correlazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Inter Agreement with Cohen Kappa Score:\n",
      "Kappa score for babelsynset id 1: 0.9185004074979625\n",
      "Kappa score for babelsynset id 2: 0.7562956945572705\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Inter Agreement with Cohen Kappa Score:\")\n",
    "ids_laura_id1, ids_laura_id2 = zip(*ids_laura)\n",
    "ids_livio_id1, ids_livio_id2 = zip(*ids_livio)\n",
    "\n",
    "kappa_id1 = cohen_kappa_score(ids_laura_id1, ids_livio_id1)\n",
    "kappa_id2 = cohen_kappa_score(ids_laura_id2, ids_livio_id2)\n",
    "\n",
    "print(\"Kappa score for babelsynset id 1:\", kappa_id1)\n",
    "print(\"Kappa score for babelsynset id 2:\", kappa_id2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Valutazione annotazione\n",
    "\n",
    "Vengono estratti i BabelSynsetID che massimizzano la similarità tra le parole e successivamente vengono memorizzati in un file con le parole associate ai synset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_nasari(pairs: list[tuple[str, str]], nasari_vectors: dict, senses2synsets: dict) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Restituisce i synset associati ai termini di ogni coppia di termini a partire dai vettori NASARI che massimizzano la \n",
    "    similarità tra le due parole.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for word1, word2 in pairs:\n",
    "        _, ids_synsets = get_similarity(word1, word2, nasari_vectors, senses2synsets)\n",
    "        ids.append(ids_synsets)\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['criptogramma', 'crittogramma', 'crittogrammi'], ['semiologia', 'Semiologi', 'semeiologia', 'semiotica', 'semiologo']), (['cittadella_universitaria', 'Campus_universitario', 'città_universitaria', 'Cittadella_universitaria', 'campus_universitario', 'campus'], ['collegio', 'accademia', 'università_di_ricerca', 'università_globale', 'politecnico', 'università', 'status_di_università', 'istituto', 'ateneo', 'università_ricerca']), (['autodeterminazione', 'libertà_personale', 'sovranità', 'Frēodōm', 'indipendenza', 'libertà_individuale', 'libertà'], ['autodeterminazione', 'libertà_personale', 'sovranità', 'Frēodōm', 'indipendenza', 'libertà_individuale', 'libertà'])]\n"
     ]
    }
   ],
   "source": [
    "ids_nasari = get_ids_nasari(pairs, nasari_vectors, senses2synsets)\n",
    "\n",
    "terms_ids_nasari = get_terms_synsets(ids_nasari)\n",
    "\n",
    "print(terms_ids_nasari[:3])\n",
    "\n",
    "with open(path_ids_NASARI, 'w', newline='', encoding='utf8') as file: \n",
    "    tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"BS1\", \"BS2\", \"Terms_in_BS1\", \"Terms_in_BS2\"])\n",
    "    for pair, id_nasari, terms_id_nasari in zip(pairs, ids_nasari, terms_ids_nasari):\n",
    "        terms_id0 = ','.join(terms_id_nasari[0][:3])\n",
    "        terms_id1 = ','.join(terms_id_nasari[1][:3])\n",
    "        tsv_writer.writerow([pair[0], pair[1], id_nasari[0], id_nasari[1], terms_id0, terms_id1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene effettuata la valutazione sulla base di quanti id sono diversi nelle valutazioni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_single(ids1: list[str], ids2: list[str]) -> int: \n",
    "    score = 0\n",
    "    for id1, id2 in zip(ids1, ids2):\n",
    "        if id1 != id2:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "def get_evaluation_multiple(ids1: list[tuple[str, str]], ids2: list[tuple[str, str]]) -> int:\n",
    "    score = 0\n",
    "    for id1, id2 in zip(ids1, ids2):\n",
    "        if id1[0] != id2[0] or id1[1] != id2[1]:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Evaluation with Accuracy:\n",
      "Laura's evaluation for babelsynset id 1: 0.6\n",
      "Laura's evaluation for babelsynset id 2: 0.45999999999999996\n",
      "Livio's evaluation for babelsynset id 1: 0.6599999999999999\n",
      "Livio's evaluation for babelsynset id 2: 0.5\n",
      "##### Evaluation with Accuracy (multiple):\n",
      "Laura's evaluation for pairs babelsynset: 0.30000000000000004\n",
      "Livio's evaluation for pairs babelsynset: 0.33999999999999997\n"
     ]
    }
   ],
   "source": [
    "ids_nasari_id1, ids_nasari_id2 = zip(*ids_nasari)\n",
    "\n",
    "print(\"##### Evaluation with Accuracy:\")\n",
    "\n",
    "ev_id1_laura = get_evaluation_single(ids_laura_id1, ids_nasari_id1)\n",
    "ev_id2_laura = get_evaluation_single(ids_laura_id2, ids_nasari_id2)\n",
    "ev_id1_livio = get_evaluation_single(ids_livio_id1, ids_nasari_id1)\n",
    "ev_id2_livio = get_evaluation_single(ids_livio_id2, ids_nasari_id2)\n",
    "\n",
    "print(\"Laura's evaluation for babelsynset id 1:\", 1 - ev_id1_laura/len(ids_laura_id1))\n",
    "\n",
    "print(\"Laura's evaluation for babelsynset id 2:\", 1 - ev_id2_laura/len(ids_laura_id2))\n",
    "\n",
    "print(\"Livio's evaluation for babelsynset id 1:\", 1 - ev_id1_livio/len(ids_livio_id1))\n",
    "\n",
    "print(\"Livio's evaluation for babelsynset id 2:\", 1 - ev_id2_livio/len(ids_livio_id2))\n",
    "\n",
    "print(\"##### Evaluation with Accuracy (multiple):\")\n",
    "\n",
    "ev_ids_laura = get_evaluation_multiple(ids_laura, ids_nasari)\n",
    "ev_ids_livio = get_evaluation_multiple(ids_livio, ids_nasari)\n",
    "\n",
    "print(\"Laura's evaluation for pairs babelsynset:\", 1 - ev_ids_laura/len(ids_laura))\n",
    "print(\"Livio's evaluation for pairs babelsynset:\", 1 - ev_ids_livio/len(ids_livio))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f6057f19fd601e680b310ee2ebe0fee3e78679207250b2f4d8f20eb0597a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
