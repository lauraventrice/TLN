{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence).strip()\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords_file = open(f\"./resources/utils/stop_words__frakes_baeza-yates.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords_file.close()\n",
    "\n",
    "    stopwords_file = open(f\"./resources/utils/stop_words_1.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords_file.close()\n",
    "\n",
    "    stopwords_file = open(f\"./resources/utils/stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords_file.close()\n",
    "    \n",
    "    stopwords_list = list(set(stopwords_list))\n",
    "\n",
    "    return stopwords_list\n",
    "     \n",
    "def remove_stopwords(sentence, stopwords_list):\n",
    "    return [word for word in sentence.split() if word not in stopwords_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esercitazione su Word Sense Disambiguation: \n",
    "\n",
    "#### INPUT : \n",
    "l’input per questa esercitazione è costituito da coppie di termini contenute nel file WordSim353 (disponibile nei\n",
    "formati .tsv e .csv)\n",
    "- Il file contiene 353 coppie di termini utilizzati come testset in varie competizioni internazionali\n",
    "- A ciascuna coppia è attribuito un valore numerico [0,10], che rappresenta la similarità fra gli elementi della coppia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353 correctly loaded!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "sim_file = f\"resources/WordSim353/WordSim353.csv\"\n",
    "\n",
    "pairs = []\n",
    "with open(sim_file, 'r', encoding = \"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()\n",
    "    for lines in reader:\n",
    "        pairs.append(lines)\n",
    "    \n",
    "print(\"WordSim353 correctly loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prima parte\n",
    "\n",
    "Implementare tre misure di similarità basate su WordNet. Per ciascuna delle misure di similarità, calcolare:\n",
    "- gli indici di correlazione di Spearman e\n",
    "- gli indici di correlazione di Pearson fra i risultati ottenuti e quelli ‘target’ presenti nel file annotato.\n",
    "\n",
    "Le misure da implementare sono le seguenti: \n",
    "- Wu & Palmer: \n",
    "$$cs(s_1, s_2) = \\frac{2 \\cdot depth(LCS)}{depth(s_1) + depth(s_2)}$$\n",
    "\n",
    "in cui $LCS$ rappresenta il primo antenato comune (Lowest Common Subsumer) e depth è la funzione che misura la distanza fra la radice di WordNet e il sysnet $x$. \n",
    "\n",
    "***L’obiettivo è implementare la misura di similarità di Wu & Palmer! Non dobbiamo fare ciò viene già fatto nell’implementazione di nltk o altre librerie!***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    oggetto\\n\\n    veicolo\\n\\n\\nbarca      automobile \\n\\n\\n            macchina\\n'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wu_palmer_similarity(synset1, synset2):\n",
    "    \"\"\"\n",
    "    Returns the Wu-Palmer similarity between synset.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(synset1, nltk.corpus.reader.wordnet.Synset) and isinstance(synset2, nltk.corpus.reader.wordnet.Synset):\n",
    "        lcs = synset1.lowest_common_hypernyms(synset2) # QUESTO NON SI PUO' USARE\n",
    "    else:\n",
    "        raise TypeError(\"The input parameters must be Synset objects\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    oggetto\n",
    "\n",
    "    veicolo\n",
    "\n",
    "\n",
    "barca      automobile \n",
    "\n",
    "\n",
    "            macchina\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shortest Path:\n",
    "$$sim_{path}(s_1, s_2) = 2 \\cdot depthMax - len(s_1, s_2)$$\n",
    "\n",
    "in cui $depthMax$ è un valore fissato per una specifica versione di WordNet. \n",
    "\n",
    "- Leakcock & Chodorow: \n",
    "$$sim_{LC}(s_1, s_2) = - log \\frac{len(s_1, s_2)}{2 \\cdot depthMax}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcolo della similarità\n",
    "\n",
    "Le funzioni precedentemente implementate richiedono come input sensi e non i termini. Quindi per calcolare la similarità fra 2 termini è necessario prendere la massima similarità fra tutti i sensi del primo termine e tutti i sensi del secondo termine. \n",
    "\n",
    "L'ipotesi quindi è che i due termini funzionino come contesto di disambiguazione l'uno per l'altro. \n",
    "\n",
    "L'equazione che formalizza questa idea è la seguente: \n",
    "\n",
    "$$sim(w_1, w_2) = \\max_{c_1 \\in s(w_1), c_2 \\in s(w_2)} [sim(c_1, c_2)]$$\n",
    "\n",
    "***NB!*** Per calcolare gli indici di correlazione non si è interessati a entrare nel merito di come sono calcolati, possiamo prendere delle funzioni prefatte e usarle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seconda parte\n",
    "\n",
    "Implementare l’algoritmo di Lesk (NON (!=) usare implementazione esistente, e.g., in nltk…).\n",
    "1. Estrarre 50 frasi dal corpus SemCor (corpus annotato con i synset di\n",
    "WN) e disambiguare (almeno) un sostantivo per frase. \n",
    "Calcolare l’accuratezza del sistema implementato sulla base dei sensi annotati in\n",
    "SemCor.\n",
    "    - SemCor è disponibile all’URL http://web.eecs.umich.edu/~mihalcea/downloads.html\n",
    "2. Randomizzare la selezione delle 50 frasi e la selezione del termine da disambiguare, \n",
    "e restituire l’accuratezza media su (per esempio) 10 esecuzioni del programma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_of_words(phrase):\n",
    "    \"\"\"\n",
    "    Returns the set of words of a phrase.\n",
    "    \"\"\"\n",
    "    phrase = remove_punctuation(phrase)\n",
    "    set_of_words = remove_stopwords(phrase, stop_words)\n",
    "    \n",
    "    return set_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_of_phrase(sentence):\n",
    "    \"\"\"\n",
    "    Returns the context of a phrase.\n",
    "    \"\"\"\n",
    "    sentence = remove_punctuation(sentence)\n",
    "    set_of_words = remove_stopwords(sentence, stop_words)\n",
    "    return set_of_words\n",
    "\n",
    "def compute_overlap(signature, context):\n",
    "    \"\"\"\n",
    "    returns the number of words in common between signature and context\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_words_in_common = len(list(set(signature) & set(context)))\n",
    "\n",
    "    return number_of_words_in_common\n",
    "\n",
    "def get_signature(synset):\n",
    "    \"\"\"\n",
    "    returns the signature of synset\n",
    "    \"\"\"\n",
    "\n",
    "    gloss_of_synset = synset.definition() #gloss of synset\n",
    "    examples_of_synset = synset.examples() #examples of synset\n",
    "\n",
    "    # print(\"\\ngloss_of_synset\")\n",
    "    # print(gloss_of_synset)\n",
    "\n",
    "    # print(\"examples_of_synset\")\n",
    "    # print(examples_of_synset)\n",
    "\n",
    "    initial_signature = [gloss_of_synset] + examples_of_synset\n",
    "\n",
    "    signature = []\n",
    "    for phrase in initial_signature:\n",
    "        set_of_words = get_set_of_words(phrase)\n",
    "        signature.append(set_of_words)\n",
    "\n",
    "    result = sum(signature, []) # flatten list of lists\n",
    "    return list(set(result)) # remove duplicates\n",
    "\n",
    "def lesk(word, sentence):\n",
    "    \"\"\"\n",
    "    returns best sense of word\n",
    "    \"\"\"\n",
    "\n",
    "    best_sense = None #most frequent sense for word\n",
    "    max_overlap = 0\n",
    "    context = get_context_of_phrase(sentence) #set of words in sentence\n",
    "\n",
    "    print(\"CONTEXT:\")\n",
    "    print(context)\n",
    "\n",
    "    for synset in wn.synsets(word):\n",
    "        print(\"\\n\" + str(synset))\n",
    "        signature = get_signature(synset) #set of words in the gloss and examples of sense\n",
    "\n",
    "        print(\"SIGNATURE:\")\n",
    "        print(signature)\n",
    "\n",
    "        overlap = compute_overlap(signature, context)\n",
    "        print(\"OVERLAP: \" + str(overlap) + \"\\n\")\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = synset\n",
    "\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT:\n",
      "['I', 'dog', 'park']\n",
      "\n",
      "Synset('dog.n.01')\n",
      "SIGNATURE:\n",
      "['member', 'genus', 'domesticated', 'common', 'barked', 'Canis', 'times', 'occurs', 'breeds', 'descended', 'wolf', 'dog', 'night', 'man', 'prehistoric']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('frump.n.01')\n",
      "SIGNATURE:\n",
      "['girl', 'dull', 'frump', 'dog', 'unattractive', 'real', 'unpleasant', 'woman', 'reputation']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('dog.n.03')\n",
      "SIGNATURE:\n",
      "['informal', 'term', 'lucky', 'dog', 'man']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('cad.n.01')\n",
      "SIGNATURE:\n",
      "['dirty', 'reprehensible', 'morally', 'dog']\n",
      "OVERLAP: 1\n",
      "\n",
      "\n",
      "Synset('frank.n.02')\n",
      "SIGNATURE:\n",
      "['sausage', 'served', 'minced', 'smoothtextured', 'smoked', 'roll', 'bread', 'pork', 'beef']\n",
      "OVERLAP: 0\n",
      "\n",
      "\n",
      "Synset('pawl.n.01')\n",
      "SIGNATURE:\n",
      "['notch', 'ratchet', 'wheel', 'forward', 'hinged', 'backward', 'fits', 'catch', 'moving', 'move', 'prevent']\n",
      "OVERLAP: 0\n",
      "\n",
      "\n",
      "Synset('andiron.n.01')\n",
      "SIGNATURE:\n",
      "['fireplace', 'andirons', 'logs', 'touch', 'supports', 'hot', 'metal']\n",
      "OVERLAP: 0\n",
      "\n",
      "\n",
      "Synset('chase.v.01')\n",
      "SIGNATURE:\n",
      "['intent', 'dog', 'The', 'mugger', 'catch', 'alley', 'rabbit', 'policeman', 'chased']\n",
      "OVERLAP: 1\n",
      "\n",
      "best sense:\n",
      "Synset('dog.n.01')\n"
     ]
    }
   ],
   "source": [
    "best_sense = lesk(\"dog\", \"I saw a dog in the park\")\n",
    "\n",
    "print(\"best sense:\")\n",
    "print(best_sense) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1840fd1808eb491d43832dc3068188bb4de4861fc71046206c0ee2dff849d2e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
